apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: prometheus-operator
    role: alert-rules
    release: prometheus-operator
  name: prometheus-operator-custom-kubernetes-apps.rules
spec:
  groups:
  - name: kubernetes-apps
    rules:
    - alert: KubeAPILatencyWarning
      annotations:
        message: The API server has an abnormal latency of {{ $value }} seconds for {{ $labels.verb
          }} {{ $labels.resource }}.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
      expr: |-
        (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver",resource!="ingresses",verb!="POST"}
        > on(verb) group_left() (avg by(verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"}
        >= 0) + 2 * stddev by(verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"}
        >= 0))) > on(verb) group_left() 1.2 * avg by(verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"}
        >= 0) and on(verb, resource) cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99"}
        > 1
      for: 5m
      labels:
        severity: warning
    - alert: KubeAPILatencyCritical
      annotations:
        message: The API server has a 99th percentile latency of {{ $value }} seconds for
          {{ $labels.verb }} {{ $labels.resource }}.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
      expr: |-
        cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99",resource!="ingresses",verb!="POST"} > 4
      for: 10m
      labels:
        severity: critical
    - alert: KubeAPILatencyWarning-IngressPost
      annotations:
        message: The API server has an abnormal latency of {{ $value }} seconds for {{ $labels.verb
          }} {{ $labels.resource }}.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
      expr: |-
        (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver",resource="ingresses",verb="POST"}
        > on(verb) group_left() (avg by(verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"}
        >= 0) + 2 * stddev by(verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"}
        >= 0))) > on(verb) group_left() 1.2 * avg by(verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"}
        >= 0) and on(verb, resource) cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99"}
        > 30
      for: 5m
      labels:
        severity: warning
    - alert: KubeAPILatencyCritical-IngressPost
      annotations:
        message: The API server has a 99th percentile latency of {{ $value }} seconds for
          {{ $labels.verb }} {{ $labels.resource }}.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
      expr: |-
        cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99",resource="ingresses",verb="POST"} > 50
      for: 10m
      labels:
        severity: critical
    - alert: KubeQuota-Exceeded
      annotations:
        message: Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value
          }}% of its {{ $labels.resource }} quota.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded
      expr: |-
        100 * kube_resourcequota{job="kube-state-metrics", type="used"} 
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"}
        / ignoring(instance, job, type)
        (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
        > 90
      for: 15m
      labels:
        severity: warning
    - alert: KubePodCrashLooping
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
          }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
      expr: |-
        rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m]) 
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"}
        * 60 * 5 > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubePodNotReady
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
          state for longer than an hour.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: |-
        sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown"}) 
        * on (namespace) 
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"} 
        > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        message: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
          }} does not match, this indicates that the Deployment has failed but has
          not been rolled back.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
      expr: |-
        kube_deployment_status_observed_generation{job="kube-state-metrics"}
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"} 
        !=
        kube_deployment_metadata_generation{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeDeploymentReplicasMismatch
      annotations:
        message: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not
          matched the expected number of replicas for longer than an hour.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
      expr: |-
        kube_deployment_spec_replicas{job="kube-state-metrics"} 
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"} 
        != kube_deployment_status_replicas_available{job="kube-state-metrics"}
      for: 1h
      labels:
        severity: warning
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has
          not matched the expected number of replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
      expr: |-
        kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"}
        !=
        kube_statefulset_status_replicas{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        message: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
          }} does not match, this indicates that the StatefulSet has failed but has
          not been rolled back.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
      expr: |-
        kube_statefulset_status_observed_generation{job="kube-state-metrics"}        
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"}
        !=
        kube_statefulset_metadata_generation{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: critical
    - alert: KubeStatefulSetUpdateNotRolledOut
      annotations:
        message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
          has not been rolled out.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
      expr: |-
        max without (revision) (
          kube_statefulset_status_current_revision{job="kube-state-metrics"}
          * on (namespace)
          group_left() 
          kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"}
            unless
          kube_statefulset_status_update_revision{job="kube-state-metrics"}
        )
          *
        (
          kube_statefulset_replicas{job="kube-state-metrics"}
          * on (namespace)
          group_left() 
          kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"}
          !=
          kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
        )  
      for: 15m
      labels:
        severity: warning
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        message: Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace
          }}/{{ $labels.daemonset }} are scheduled and ready.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
      expr: |-
        kube_daemonset_status_number_ready{job="kube-state-metrics"}
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"}
        /
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"} * 100 < 100
      for: 15m
      labels:
        severity: warning
    - alert: KubeDaemonSetNotScheduled
      annotations:
        message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
          }} are not scheduled.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
      expr: |-
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}     
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"}
        -
        kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0 
      for: 10m
      labels:
        severity: warning
    - alert: KubeDaemonSetMisScheduled
      annotations:
        message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
          }} are running where they are not supposed to run.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
      expr: |-
        kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} 
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"} 
        > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeCronJobRunning
      annotations:
        message: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more
          than 1h to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecronjobrunning
      expr: |-
        time() - kube_cronjob_next_schedule_time{job="kube-state-metrics"} 
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"} 
        > 3600
      for: 1h
      labels:
        severity: warning
    - alert: KubeJobCompletion
      annotations:
        message: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more
          than one hour to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
      expr: |-
        kube_job_spec_completions{job="kube-state-metrics"} - kube_job_status_succeeded{job="kube-state-metrics"} 
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"} 
        > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubeJobFailed
      annotations:
        message: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
      expr: |-
        kube_job_status_failed{job="kube-state-metrics"}  
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"} 
        > 0
      for: 1h
      labels:
        severity: warning
    - alert: TargetDown
      annotations:
        message: '{{ $value }}% of the {{ $labels.job }} targets are down.'
      expr: |-
        100 * (count(up == 0) BY (job) / count(up) BY (job)) 
        * on (namespace)
        group_left() 
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"} 
        > 10
      for: 10m
      labels:
        severity: warning
    - alert: KubePersistentVolumeFullInFourDays-Low
      annotations:
        message: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ printf "%0.2f" $value }} is available.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays
      expr: |-
        100 * (
          kubelet_volume_stats_available_bytes{job="kubelet"}
            /
          kubelet_volume_stats_capacity_bytes{job="kubelet"}
        ) < 15
        and
        predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600)
        * on (namespace)
        group_left()
        kube_namespace_annotations{annotation_cloud_platform_out_of_hours_alert="true"} < 0
      for: 5m
      labels:
        severity: warning
    - alert: VeleroBackupPartialFailure-velero-allnamespacebackup
      annotations:
        message: A Velero backup partial failure in past 6 hours - velero-allnamespacebackup
        runbook_url: https://runbooks.cloud-platform.service.justice.gov.uk/disaster-recovery-scenarios.html#resolving-a-partiallyfailed-backup-alert
      expr: sum(increase(velero_backup_partial_failure_total{schedule="velero-allnamespacebackup"}[3h])) > 1
      for: 1m
      labels:
        severity: warning
    - alert: VeleroBackupFailure-velero-allnamespacebackup
      annotations:
        message: A Velero backup failure in past 6 hours - velero-allnamespacebackup
        runbook_url: https://runbooks.cloud-platform.service.justice.gov.uk/disaster-recovery-scenarios.html#resolving-a-partiallyfailed-backup-alert
      expr: sum(increase(velero_backup_failure_total{schedule="velero-allnamespacebackup"}[3h])) > 1
      for: 1m
      labels:
        severity: warning
    - alert: VeleroBackupNotSuccessfulForOverEightHours-velero-allnamespacebackup
      annotations:
        message: The Velero backup schedule for AllNamespaceBackup does not have a successful timestamp for over 8 hours
        runbook_url: https://runbooks.cloud-platform.service.justice.gov.uk/disaster-recovery-scenarios.html#resolving-a-partiallyfailed-backup-alert
      expr: (time() - velero_backup_last_successful_timestamp{schedule="velero-allnamespacebackup"}) / 60 / 60 > 8
      for: 1m
      labels:
        severity: warning
    - alert: VeleroBackupPartialFailure
      annotations:
        message: A Velero backup partial failure notification
        runbook_url: https://runbooks.cloud-platform.service.justice.gov.uk/disaster-recovery-scenarios.html#resolving-a-partiallyfailed-backup-alert
      expr: sum(increase(velero_backup_partial_failure_total[2m])) > 0
      for: 1m
      labels:
        severity: info-warning
    - alert: VeleroBackupFailure
      annotations:
        message: A Velero backup failure notification
        runbook_url: https://runbooks.cloud-platform.service.justice.gov.uk/disaster-recovery-scenarios.html#resolving-a-partiallyfailed-backup-alert
      expr: sum(increase(velero_backup_failure_total[2m])) > 0
      for: 1m
      labels:
        severity: info-warning
    - alert: coreDNSLatencyWarning
      annotations:
        message: The coreDNS for the Kubernetes cluster has a 99th percentile latency of {{ $value }} seconds for
          {{ $labels.verb }} {{ $labels.resource }}. The high latency of the request could mean a degradation in the Kubernetes cluster service. This check is comparing the percentile against the average using the operator histogram
        runbook_url: https://sysdig.com/blog/how-to-monitor-coredns/
      expr: |-
        histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket{job="coredns"}[5m])) by(server, zone, le)) > 1
      for: 10m
      labels:
        severity: warning        
    - alert: coreDNSLatencyCritical
      annotations:
        message: The coreDNS for the Kubernetes cluster has a 99th percentile latency of {{ $value }} seconds for
          {{ $labels.verb }} {{ $labels.resource }}. The high latency of the request could mean a degradation in the Kubernetes cluster service. This check is comparing the percentile against the average using the operator histogram
        runbook_url: https://sysdig.com/blog/how-to-monitor-coredns/
      expr: |-
        histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket{job="coredns"}[5m])) by(server, zone, le)) > 4
      for: 10m
      labels:
        severity: critical
    - alert: coreDNSDeploymentReplicasMismatch
      annotations:
        message: CoreDNS deployment has {{ $value }} ready replicas but should have {{ $labels.spec_replicas }}. Cluster networking may be impacted. Check for any Pending pods or unhealthy cluster nodes. Consider rollout restart of CoreDNS deployment.
        runbook_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/k8s_system_coredns/kubernetes-system-coredns?orgId=1&from=now-1h&to=now&timezone=browser&var-datasource=prometheus&var-instance=$__all&var-protocol=udp&var-resolution=30s&refresh=30s
      expr: |-
        kube_deployment_status_replicas_ready{deployment="coredns"} 
        != 
        kube_deployment_spec_replicas{deployment="coredns"}
      for: 5m
      labels:
        severity: warning
    - alert: PrometheusServerMemHigh
      annotations:
        message: The Prometheus server uses a lot of memory and has ocassionally OOMKilled bringing down prometheus. Bump the node group monitoring instance size
        runbook_url: https://github.com/ministryofjustice/cloud-platform-infrastructure/commit/3dc05e588c9115c7aa44c2a9b5e26feff985f965
      expr: |-
        sum(container_memory_working_set_bytes{namespace="monitoring", pod="prometheus-prometheus-operator-kube-p-prometheus-0", container="prometheus"}) / sum(kube_pod_container_resource_limits{namespace="monitoring", pod="prometheus-prometheus-operator-kube-p-prometheus-0", resource="memory", container="prometheus"}) * 100 > 80
      for: 5m
      labels:
        severity: warning
    - alert: PrometheusServerMemCritical
      annotations:
        message: The Prometheus server uses a lot of memory and has ocassionally OOMKilled bringing down prometheus. Bump the node group monitoring instance size
        runbook_url: https://github.com/ministryofjustice/cloud-platform-infrastructure/commit/3dc05e588c9115c7aa44c2a9b5e26feff985f965
      expr: |-
        sum(container_memory_working_set_bytes{namespace="monitoring", pod="prometheus-prometheus-operator-kube-p-prometheus-0", container="prometheus"}) / sum(kube_pod_container_resource_limits{namespace="monitoring", pod="prometheus-prometheus-operator-kube-p-prometheus-0", resource="memory", container="prometheus"}) * 100 > 90
      for: 2m
      labels:
        severity: critical
    - alert: External-DNSDown
      expr: kube_deployment_status_replicas_available{deployment="external-dns"} == 0
      for: 5m
      labels:
        severity: warning
      annotations: 
        message: external-dns container has not been running in the namespace 'kube-system' for 5 minutes.
        runbook_url: https://github.com/ministryofjustice/cloud-platform-terraform-monitoring/blob/main/resources/prometheusrule-alerts/README.md#kubednsdown
    - alert: CanaryAppCPUHigh
      annotations:
        message: CPU throttling detected in the {{ $labels.namespace }} namespace. Please investigate and increase CPU limts if neccesary. 
        runbook_url: https://github.com/ministryofjustice/cloud-platform-environments/pull/22575/commits/d821477fa2b30649a12573ae0a5eecced7fbb213
      expr: |-
        sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{namespace="cloud-platform-canary-app-eks"}) / sum(kube_pod_container_resource_limits{ namespace="cloud-platform-canary-app-eks", resource="cpu"}) * 100 > 80
      for: 5m
      labels:
        severity: warning
    - alert: ThanosCompactorCPUHigh
      annotations:
        message: CPU throttling detected in the {{ $labels.namespace }} namespace. Please investigate and increase CPU limts if neccesary.
          https://github.com/ministryofjustice/cloud-platform-terraform-monitoring/pull/241/files
        runbook_url: https://github.com/ministryofjustice/cloud-platform-terraform-monitoring/pull/241/files
      expr: |-
        sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{clusterName="manager", namespace="monitoring", container="compactor"}) / sum(kube_pod_container_resource_limits{clusterName="manager", namespace="monitoring", container="compactor", resource="cpu"}) * 100 > 80
      for: 5m
      labels:
        severity: warning
    - alert: ThanosCompactorMemHigh
      annotations:
        message: The Thanos Compactor is usuing a lot of memory and could be OOMKilled. Please investigate and increase the container MEM limits if neccesary.
        runbook_url: https://github.com/ministryofjustice/cloud-platform-terraform-monitoring/pull/241/files
      expr: |-
        sum(container_memory_working_set_bytes{clusterName="manager", namespace="monitoring", container="compactor"}) / sum(kube_pod_container_resource_limits{clusterName="manager", namespace="monitoring", container="compactor", resource="memory"}) * 100 > 80
      for: 5m
      labels:
        severity: warning
    - alert: ThanosCompactorHalted
      annotations:
        message: The Thanos Compactor has encounted errors compacting and halted, please investigate
        runbook_url: https://github.com/ministryofjustice/cloud-platform-terraform-monitoring/pull/241/files
      expr: |-
        sum(thanos_compact_halted) > 0
      for: 5m
      labels:
        severity: warning

